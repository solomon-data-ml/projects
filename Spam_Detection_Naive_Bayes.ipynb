{"cells":[{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style=\"font-size:18px\"><span style=\"color:#2f4f4f\"><strong><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Spam detection - Multinomial Naive Bayes</span></strong></span></span></p>"},{"metadata":{},"cell_type":"markdown","source":"<p><u><span style=\"color:#a52a2a\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Below are the steps that we are going to do:</span></span></u></p>\n\n<p style=\"margin-left: 40px;\"><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">1. Read data from the dataset</span></span></p>\n\n<p style=\"margin-left: 40px;\"><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">2. Remove the empty columns v3,v4,v5&nbsp;</span></span></p>\n\n<p style=\"margin-left: 40px;\"><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">3. Remove the first row which contains v1, v2 which is not releveant to our dataset</span></span></p>\n"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nspam = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding = \"ISO-8859-1\",names=[\"label\",\"message\",\"v3\",\"v4\",\"v5\"])\ndel spam[\"v3\"]\ndel spam[\"v4\"]\ndel spam[\"v5\"]\nspam = spam.drop(spam.index[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Just to confirm if there are any invalid data, analyze the unique target variable( in our case the target column is the &quot;label&quot;)&nbsp;, and the result is the expected &quot;ham&quot; and &quot;spam&quot;; &nbsp;so we can proceed further</span></span></p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.label.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><u><span style=\"color:#a52a2a\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Iterate through all the messages and do the following:</span></span></u></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">a) Remove all the characters except a to z and A to Z</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">b) Convert all the characters to lower case so that there cant be any duplicates</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">c) Split the sentences to get the list of words&nbsp;</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">d) for each of the word in a Sentense, stem that (get the base word) and remove all the words if that exists in stop words &nbsp;</span></span></p>\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">   (Stop words are like the, is, was, those, these,... e.t.c) &nbsp;</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">&nbsp;e) The final output will be the messages that contains the&nbsp;result of step a to d&nbsp;</span></span></p>\n\n<p>&nbsp; &nbsp; &nbsp;&nbsp;</p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are interating from index 1 because the zero index row is already deleted in previous step.\ncorpus = []\nfor i in range(1, len(spam)+1):\n    review  = re.sub('[^a-zA-Z]',' ',spam['message'][i])\n    review  = review.lower()\n    review  = review.split()\n    review  = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review  = ' '.join(review)\n    corpus.append(review)\n    \nprint(corpus[:10])  \n \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Create vectorizer that takes the top 5000 most frequent words</span></span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":" from sklearn.feature_extraction.text import CountVectorizer\n cv = CountVectorizer(max_features=5000)\n x = cv.fit_transform(corpus).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">The target column &quot;label&quot; contains text data which is &quot;ham&quot; or &quot;spam&quot;,&nbsp;</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">we need to convert this to numerical value for processing.&nbsp;</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">get_dummies will do this for us, creates two columns for each category.</span></span></p>\n\n<p><span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">We can choose any one of them using iloc</span></span></p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.get_dummies(spam[\"label\"])\ny = y.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Split the data into training and test sets</span></span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Fit using Naive Bayes</span></span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>&nbsp; <span style=\"color:#000080\"><span style=\"font-family:Lucida Sans Unicode,Lucida Grande,sans-serif\">Use the test data to predict , check the confusion matrix and observe the accuracy</span></span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = spam_detect_model.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_m = confusion_matrix(y_test,y_pred)\nprint(confusion_m)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}