{"cells":[{"metadata":{},"cell_type":"markdown","source":" <h1><span style=\"color: #808080;\">BOSTON Dataset Analysis</span></h1>"},{"metadata":{},"cell_type":"markdown","source":"The objective is to understand the boston dataset which is available in sklearn kit"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_boston\nimport pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><strong><span style=\"color: #33cccc;\">Load the boston dataset</span></strong></p> "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ds = load_boston()\nds.data.shape ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><strong><span style=\"color: #33cccc;\">Study the columns</span></strong></p> "},{"metadata":{"trusted":true},"cell_type":"code","source":" \nprint(ds.feature_names)\n\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><strong><span style=\"color: #33cccc;\">Create pandas dataframe and have a split as features and target</span></strong></p> "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f = pd.DataFrame(data=ds.data,columns=ds.feature_names)\ndata_t = pd.DataFrame(data=ds.target,columns=['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><strong><span style=\"color: #33cccc;\">Check the data type of the feature and target </span></strong></p> "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f.info();\ndata_t.info();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><strong><span style=\"color: #33cccc;\">Check the data type of the feature and target check if there are any missing values in the columns</span></strong></p> "},{"metadata":{"trusted":true},"cell_type":"code","source":" pd.isnull(data_f).any()\n pd.isnull(data_t).any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><strong><span style=\"color: #33cccc;\">Lets us do some univariate analysis</span></strong></p> "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.set(rc={'axes.facecolor':'grey', 'figure.facecolor':'grey'})\nsns.distplot(data_t['target'], bins = 50);\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : We can see the outliers to the right, but pretty much gaussion "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f['RM'].mean()\ndata_f['RAD'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation :**\n1. *On an average, an individual has 6 rooms. But is that a valid argument? seems to have outliers\n2. *The RAD index values defines how much accessiblity it is in terms of road, outlets, food e.t.c\n   7 means low accessibility and 24 means high accessibility. We can see from the above \n   that there are more datas(which implies houses) which are highly accessible.*\n   "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"data_t[\"target\"].corr(data_f[\"RM\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a new dataset which includes the target column for analyzing correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = data_f.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all[\"PRICE\"] = data_t[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_all.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating filter to remove symmetric in the heat map\nfilter = np.zeros_like(data_all.corr())\ntri_indice = np.triu_indices_from(filter)\nfilter[tri_indice]=True\nfilter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15));\nsns.set_style('dark');\nsns.heatmap(data_all.corr(),annot=True,mask=filter);\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><span style=\"color: rgb(26, 188, 156);\"><strong>Observation:</strong></span></p>\n<pre><span style=\"color: rgb(147, 101, 184); font-size: 12px;\">1. Correlation between Tax and RAD is high. </span><span style=\"color: rgb(147, 101, 184);\"><span style=\"font-size: 12px;\">\n2. The lowest correlated for the price is CHAS. Chas is a dummy variable so it can be ignored.\n</span></span><span style=\"color: rgb(147, 101, 184); font-size: 12px;\">3. The second lowest correlation with price is DIS , but much correlated with industry features.</span></pre>\n<p>&nbsp;</p>\n<p><strong><span style=\"color: rgb(26, 188, 156);\">Note:</span></strong></p>\n<p><span style=\"color: rgb(147, 101, 184); font-size: 11px;\">This is Pearson correlation which is not good for continuous data</span></p>\n<p><span style=\"color: rgb(147, 101, 184);\"><span style=\"caret-color: rgb(128, 128, 128); font-size: 11px;\">This will work only on linear data</span></span>\n</p>"},{"metadata":{},"cell_type":"markdown","source":"<pre><span style=\"font-family: Tahoma, Geneva, sans-serif; font-size: 14px; color: rgb(84, 172, 210);\"><strong>Relation between the Pollution and the Distance from employment center </strong></span><span style=\"color: rgb(84, 172, 210);\"><span style=\"font-size: 14px;\">\n<span style=\"font-family: Tahoma,Geneva,sans-serif;\">NOX - MEASURE OF POLLUTION</span>\n</span></span><span style=\"font-family: Tahoma, Geneva, sans-serif; font-size: 14px; color: rgb(84, 172, 210);\">DIS  - MEASURE OF DISTANCE FROM EMPLOYMENT CENTER</span></pre>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(data_f['NOX'].corr(data_f['DIS']))\nplt.scatter(x=data_f['DIS'], y=data_f['NOX'],alpha=0.6);\nplt.xlabel('Distance from Employment Center');\nplt.ylabel('Measure of pollution');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously those which are near to emplyment will be high than the suburbs"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_all,kind='reg', plot_kws = {'line_kws' : {'color':'red'}})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all[\"INDUS\"].corr(data_all[\"PRICE\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <pre><font color=\"#9365b8\"><span style=\"color: rgb(44, 130, 201); font-size: 12px;\">For price we can see there is good relationship with Indus, RM, LSTAT (This one fits well with regression line), The property price seems to be lower where there are more people who are not having high <span style=\"caret-color: rgb(147, 101, 184);\">school</span> education.</span></font><span style=\"color: rgb(44, 130, 201);\"><span style=\"font-size: 12px;\">\n </span></span></pre>\n<pre><span style=\"color: rgb(44, 130, 201);\"><span style=\"font-size: 12px;\">Observe in correlation matrix that the LSTAT and INDUS value is high, means\n</span></span><span style=\"color: rgb(44, 130, 201); font-size: 12px;\">the number of the industries are more in the area where the people who are classified as not having school eduction(LSTAT).</span></pre>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_f, data_t,test_size=0.25) # meaning 20% will be test data.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(\"===================================================\")\nprint(\" r squared of train : \", lr.score(X_train,y_train))\nprint(\"  r squared of test : \", lr.score(X_test,y_test))\nprint (\"        Y Intercept : \" , lr.intercept_)\nprint(\"===================================================\")\nlr.coef_.shape\npd.DataFrame(data = lr.coef_,columns=X_train.columns);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <span style=\"color: rgb(44, 130, 201); font-size: 12px;\">Cross verify the above coefficents with the one we already done in pair plot regression lines.</span>\n <div><span style=\"color: rgb(44, 130, 201); font-size: 12px;\"></br>Now, when we see the distribution of Price vs Number of houses , it is right skewed, and this will not make the model better.  </span></div>\n <div><span style=\"color: rgb(44, 130, 201); font-size: 12px;\">To resolve this, we can update all the values to Log of the same which will make the model better fit for linear equation. </span></div>\n  <div><span style=\"color: rgb(44, 130, 201); font-size: 12px;\">To resolve this, we can update all the values to Log of the same which will make the model better fit for linear equation. </span></div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all['PRICE'].skew() #Zero value means no skewness","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \n<h3><span style=\"font-family:arial,helvetica,sans-serif;\">Below are the tasks we will be doing:</span></h3>\n\n<p><span style=\"font-family:arial,helvetica,sans-serif;\">1. Create a variable that stores the log of all the prices</span></p>\n\n<p><span style=\"font-family:arial,helvetica,sans-serif;\">2. Create a dist plot and compare the skewness of the graph (before log, and after log)</span></p>\n\n<p><span style=\"font-family:arial,helvetica,sans-serif;\">3. Create a regression plot and compare - before log and after log , how the data fits there</span></p>\n\n<p><span style=\"font-family:arial,helvetica,sans-serif;\">4. After getting enough confidence about the above, test and train using the log of price</span></p>\n\n<p><span style=\"font-family:arial,helvetica,sans-serif;\">5. and compare the performance(R squared value, coefficients)</span></p>\n \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"price_log = np.log(data_t['target'])\nprice_log.skew() #clearly the results are near to zero unlike previous 1.108","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2)\nsns.distplot(data_t['target'], ax=ax[0])\nsns.distplot(np.log(data_t['target']), ax=ax[1])\nfig.show();\n \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"font-family:arial,helvetica,sans-serif;\">Clearly, the outliers are now very much closer to the adjacent bins compared to the one without the log</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_f_withlog = data_f.copy()\ndata_f_withlog[\"LOG_PRICE\"] = price_log\nsns.lmplot(x=\"LSTAT\",y=\"PRICE\",data=data_all,size=7,scatter_kws={'alpha':0.6},line_kws={'color':'darkred'}  );\nsns.lmplot(x=\"LSTAT\",y=\"LOG_PRICE\",data=data_f_withlog,size=7,scatter_kws={'alpha':0.6},line_kws={'color':'darkred'} );\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"font-family:arial,helvetica,sans-serif;\">Not a very big difference but we can see a good fit at the top left top better than the first graph (without log)</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test train using log of the price variable\nX_train, X_test, y_train, y_test = train_test_split(data_f, np.log(data_t),test_size=0.25);\nlr = LinearRegression();\nlr.fit(X_train, y_train);\nprint(\"===================================================\")\nprint(\" r squared of train : \", lr.score(X_train,y_train));\nprint(\"  r squared of test : \", lr.score(X_test,y_test));\nprint (\"        Y Intercept : \" , lr.intercept_);\nprint(\"===================================================\")\n\n\n#We get better R squared value thn before","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<span style=\"font-family:arial,helvetica,sans-serif;\">Lets us find the p value of the features with respect to the price and identify are there any features that is insignificance to our model. If P value is less than 0.05 consider then we can conclude that model is not behaving actually with the statistical data we have. so we can ignore those features</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nx_include_const = sm.add_constant(X_train);\nmodel = sm.OLS(y_train, x_include_const);\noutput = model.fit();\nprint(\"======================= Parameters ============================\")\nprint(output.params);\nprint(\"======================= P Values ============================\")\nprint(round(output.pvalues,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<span style=\"font-family:arial,helvetica,sans-serif;\">So apart from INDUS and AGE , all the other features are statistically adding good value to our model</span>"},{"metadata":{},"cell_type":"markdown","source":"<span style=\"font-family:arial,helvetica,sans-serif;\">The below results also provides that there is no multi colinearity - that is below 10 Threshold</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(exog = x_include_const.values, exog_idx=i) for i in range(x_include_const.shape[1])]\npd.DataFrame({'coef_name':x_include_const.columns,'vif' : np.around(vif,4 )})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the P-values we can see the DIST is zero, which is statistically significant , but the Pv- alue of indus is 0.44 and price is 0.971 which is not statistically significant, so lets remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#With INDUS and PRICE column \nx_include_const = sm.add_constant(X_train);\nmodel = sm.OLS(y_train, x_include_const);\noutput = model.fit();\ncoef = pd.DataFrame({'params':output.params,'pvalues':round(output.pvalues,3)})\nprint(\"BIC: \",output.bic)\nprint(\"R Squared: \",output.rsquared)\nprint(\"===================== WITHOUT INDUS AND AGE FEATURES=====================\")\n#without INDUS and PRICE column\nx_exclude = x_include_const.drop(['INDUS','AGE'], axis = 1)\nmodel_exclude = sm.OLS(y_train, x_exclude);\noutput_exclude = model_exclude.fit();\ncoef_exclude = pd.DataFrame({'params':output_exclude.params,'pvalues':round(output_exclude.pvalues,3)})\nprint(\"BIC: \",output_exclude.bic)\nprint(\"R Squared: \",output_exclude.rsquared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no much difference in R squared but the Basian information is lowered without INDUS and AGE. which concludes that\nwe can remove those two features without affecting the model"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}